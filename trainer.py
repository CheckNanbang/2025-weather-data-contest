import argparse
import pandas as pd
import numpy as np
import os
from datetime import datetime
from tqdm import tqdm
import logging
import optuna

from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    mean_squared_log_error, mean_absolute_percentage_error, explained_variance_score
)

from dataloader.data_loader import data_loader
from utils.data_split import data_split
from utils.load_params import load_params
from models.train_model import train_model


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument("--dataset", type=str, default="heat")
    parser.add_argument("--model", type=str, default="XGB")
    parser.add_argument("--split_type", type=str, default="time", choices=["random", "time"])
    parser.add_argument("--model_type", type=str, default="regressor", choices=["regressor", "classifier"])
    parser.add_argument("--submit", action="store_true", help="Submission ìƒì„± ì—¬ë¶€")
    parser.add_argument("--tune", action="store_true", help="Optunaë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    parser.add_argument("--params", nargs="*", help="key=value í˜•íƒœë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…ë ¥")
    
    return parser.parse_args()


def parse_params(param_list):
    if not param_list:
        return {}
    return {k: eval(v) for k, v in (p.split('=') for p in param_list)}


def setup_logger(log_name):
    os.makedirs("logs", exist_ok=True)
    log_path = os.path.join("logs", log_name)
    logging.basicConfig(filename=log_path, level=logging.INFO, format='%(message)s')
    return logging.getLogger()


def evaluate(y_true, y_pred):
    metrics = {
        "MAE": mean_absolute_error(y_true, y_pred),
        "MSE": mean_squared_error(y_true, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "R2": r2_score(y_true, y_pred),
        "MAPE": mean_absolute_percentage_error(y_true, y_pred),
        "EVS": explained_variance_score(y_true, y_pred)
    }
    try:
        metrics["MSLE"] = mean_squared_log_error(y_true, y_pred)
    except ValueError:
        metrics["MSLE"] = None
    return metrics


def objective(trial, model_name, model_type, x_train, y_train, x_valid, y_valid):
    if model_name == "XGB":
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 500),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "max_depth": trial.suggest_int("max_depth", 3, 10)
        }
    else:
        raise NotImplementedError("Only XGB tuning implemented")

    model = train_model(model_name, model_type, params, x_train, y_train)
    preds = model.predict(x_valid)
    return mean_squared_error(y_valid, preds)


def main():
    args = parse_args()
    now = datetime.now()
    date_code = now.strftime("%m%d%H%M")
    save_name = f"{args.model}_{args.dataset}_{args.split_type}_{date_code}.csv"
    logger = setup_logger(f"{args.model}_{date_code}.log")

    # ë°ì´í„° ë¡œë”© ë° ë¶„í• 
    train_df, test_df, submission_df, target_column = data_loader(args.dataset)
    x_train, x_valid, y_train, y_valid = data_split(args.split_type, train_df, target_column)
    X_test = test_df.copy()

    # ì»¬ëŸ¼ ì‚­ì œ
    drop_cols = ["train_heatbranch_id"]
    for col in drop_cols:
        x_train = x_train.drop(columns=[col], errors="ignore")
        x_valid = x_valid.drop(columns=[col], errors="ignore")
        X_test = X_test.drop(columns=[col], errors="ignore")

    # íŒŒë¼ë¯¸í„° ì¤€ë¹„
    user_params = parse_params(args.params)
    default_params = load_params(args.model, args.model_type)
    params = {**default_params, **user_params}
    
    print("ğŸ”§ ìµœì¢… ì‚¬ìš© íŒŒë¼ë¯¸í„°:")
    for k, v in params.items():
        print(f"{k}: {v}")
        logger.info(f"{k}: {v}")

    # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    if args.tune:
        study = optuna.create_study(direction="minimize")
        study.optimize(lambda trial: objective(trial, args.model, args.model_type, x_train, y_train, x_valid, y_valid), n_trials=20)
        print("ğŸ¯ Best trial:", study.best_trial.params)
        params.update(study.best_trial.params)

    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    model = train_model(args.model, args.model_type, params, x_train, y_train)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")

    # ì˜ˆì¸¡ ë° í‰ê°€
    y_valid_pred = model.predict(x_valid)
    metrics = evaluate(y_valid, y_valid_pred)

    print("ğŸ“Š ê²€ì¦ ì§€í‘œ")
    for k, v in metrics.items():
        result = f"{k}: {v:.4f}" if v is not None else f"{k}: Not available"
        print(result)
        logger.info(result)

    # ìµœì¢… í•™ìŠµ ë° ì˜ˆì¸¡
    if args.submit:
        print("ğŸ“ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° submission ìƒì„±")
        x_total = pd.concat([x_train, x_valid], axis=0)
        y_total = pd.concat([y_train, y_valid], axis=0)
        model = train_model(args.model, args.model_type, params, x_total, y_total)
        X_test = X_test.drop(columns=[target_column], errors="ignore")
        test_pred = model.predict(X_test)
        submission_df['heat_demand'] = test_pred
        submission_df.to_csv(save_name, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_name}")
        logger.info(f"Submission saved to {save_name}")


if __name__ == "__main__":
    main()
